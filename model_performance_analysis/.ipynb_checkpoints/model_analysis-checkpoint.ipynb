{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d7e822e-7a63-4d55-bc1f-7cd7f7c1e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, confusion_matrix, recall_score, roc_auc_score, average_precision_score, f1_score\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5f9b136-235d-4dc0-b994-89bceaf5ccd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(model, file, hyperparameters, threshold):\n",
    "\n",
    "    model_num = re.findall(r'\\d+\\.?\\d*', file)[0]\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    y_true = df['y_true']\n",
    "    y_pred = df['y_pred']\n",
    "    y_pred_round = y_pred.map(lambda x: np.around(x))\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred_round, zero_division = 0)\n",
    "    recall = recall_score(y_true, y_pred_round, zero_division = 0)\n",
    "    f1 = f1_score(y_true, y_pred_round, zero_division = 0)\n",
    "    \n",
    "    cm = confusion_matrix(y_true.to_numpy(), y_pred_round.to_numpy())\n",
    "    num_TP = cm[1,1]\n",
    "    num_FP = cm[0,1]\n",
    "\n",
    "    pr_auc = average_precision_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "    idx = (y_pred_round == y_true)\n",
    "    correct_pred_confidence_avg = (1 - abs(y_true[idx] - y_pred[idx])).mean()\n",
    "    correct_pred_confidence_std = (1 - abs(y_true[idx] - y_pred[idx])).std()\n",
    "    correct_pred_confidence = str(correct_pred_confidence_avg) + \" Â± \" + str(correct_pred_confidence_std)\n",
    "\n",
    "    new_y_pred = y_pred.map(lambda x: 1 if x > threshold else 0)\n",
    "    idx = ((new_y_pred == 1) & (y_true == 0))\n",
    "    new_cm = confusion_matrix(y_true.to_numpy(), new_y_pred.to_numpy())\n",
    "    new_num_TP = new_cm[1,1]\n",
    "    new_num_FP = new_cm[0,1]\n",
    "\n",
    "    # return [model_num, precision, num_TP, num_FP, correct_pred_confidence, new_num_TP, new_num_FP]\n",
    "\n",
    "    return [model, model_num, precision, recall, f1, num_TP, num_FP, pr_auc, roc_auc,\n",
    "            correct_pred_confidence, new_num_TP, new_num_FP, hyperparameters['lr'], hyperparameters['weight_decay']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "362356f6-8ef3-4a52-8e97-b553b8c1229a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_data(model, threshold):\n",
    "    \n",
    "    directory = '/Users/gabrielgreenstein/Downloads/' + model + '/hyperparameter_optimization'\n",
    "    \n",
    "    data = [['model', 'model_num', 'precision', 'recall', 'f1', 'num_TP', 'num_FP', 'pr_auc', 'roc_auc',\n",
    "             'correct_pred_confidence', 'new_num_TP', 'new_num_FP', 'learning_rate', 'weight_decay']]\n",
    "    \n",
    "    for model_folder in os.listdir(directory):\n",
    "        model_folder_path = os.path.join(directory, model_folder)\n",
    "        if os.path.isdir(model_folder_path) and model_folder.startswith('model_'):\n",
    "\n",
    "            config_file = os.path.join(model_folder_path, 'configure.json')\n",
    "                \n",
    "            if os.path.isfile(config_file):\n",
    "                hyperparameters = json.load(open(config_file))\n",
    "\n",
    "            val_model_folder = os.path.join(model_folder_path, 'val_model_on_infer_set')\n",
    "            \n",
    "            if os.path.isdir(val_model_folder):\n",
    "                results_file = os.path.join(val_model_folder, 'results.txt')\n",
    "                \n",
    "                if os.path.isfile(results_file):\n",
    "                    data.append(compute_metrics(model, results_file, hyperparameters, threshold))\n",
    "                else:\n",
    "                    print(f\"'results.txt' not found in {val_model_folder}\")\n",
    "            else:\n",
    "                print(f\"'val_model_on_infer_set' folder not found in {model_folder_path}\")\n",
    "    df = pd.DataFrame(data[1:], columns=data[0])\n",
    "    df = df[df['precision'] >= 0.85]\n",
    "    df.sort_values(by=['precision'], inplace=True, ascending=False)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a412c-1f13-4dec-bfdf-71d7e4171387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = ['AttentiveFP', 'Weave', 'MPNN', 'GAT']\n",
    "models = ['MPNN']\n",
    "threshold = 0.99\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for model in models:\n",
    "    result = analyze_data(model, threshold)  # Assuming this returns a DataFrame\n",
    "    df = pd.concat([df, result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c41c403-e3eb-4792-baf6-7deb4c501d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_num</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>num_TP</th>\n",
       "      <th>num_FP</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>correct_pred_confidence</th>\n",
       "      <th>new_num_TP</th>\n",
       "      <th>new_num_FP</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, model_num, precision, recall, f1, num_TP, num_FP, pr_auc, roc_auc, correct_pred_confidence, new_num_TP, new_num_FP, learning_rate, weight_decay]\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['pr_auc'], ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdf7decf-96fa-41e6-ad88-8e746b099517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('best_models.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9523d43-2b11-4768-9aeb-67f70f01eae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

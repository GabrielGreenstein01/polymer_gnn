{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f7acab8-fcfa-4fb8-b541-df39a097f589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Cores:8\n",
      "# GPUs: 0\n",
      "Available GPUs: []\n",
      "Visible GPUs Indices: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import joblib\n",
    "\n",
    "num_cores = os.cpu_count()\n",
    "num_GPUs = torch.cuda.device_count()\n",
    "\n",
    "print('# Cores:' + str(num_cores))\n",
    "print('# GPUs: ' + str(num_GPUs))\n",
    "\n",
    "# Get the available GPUs directly as a list\n",
    "print(f\"Available GPUs: {list(range(torch.cuda.device_count()))}\")\n",
    "\n",
    "print('Visible GPUs Indices: ' + str(os.environ.get('CUDA_VISIBLE_DEVICES', 'All GPUs are visible')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "354ee020-baf4-47bf-861d-fbff30368ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b5a1f99-6e41-467c-a73b-9bc63b6ddffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.create_dataset_class import DataSet\n",
    "from utils.multiclass_NN import multiclass_NN\n",
    "from utils.split_dataset import split\n",
    "from utils.scale_graph_features import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aab5ba7e-d29b-4b63-9ee5-ad298ade9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 103 # seed for splitting train/val/test set\n",
    "db_file = 'data_preprocessing/db.csv' # file containing \n",
    "\n",
    "# See comments in box below\n",
    "SPLIT_RATIO = '0.4,0.3,0.3' # Train, Val, Test\n",
    "# SPLIT_RATIO = '0.7,0.3' # Train, Val, Test\n",
    "\n",
    "SMILES = 'data_preprocessing/SMILES.txt'\n",
    "DESCRIPTORS = 'monomer_data/unique_descriptors.json'\n",
    "\n",
    "LABELNAME = 'binary_class' # label name used in db_file\n",
    "TASK = 'classification' # task (LEAVE AS IS) - currently does not support regression or multiclass classification\n",
    "MODEL = 'MPNN' # Model type ('MPNN', 'GAT', 'Weave', 'GCN', 'AttentiveFP')\n",
    "\n",
    "NUM_EPOCHS = 4 # Number of epochs to tran\n",
    "NUM_WORKERS = num_cores\n",
    "\n",
    "MODEL_PATH = 'past_trials/' + MODEL + '/' + str(NUM_EPOCHS) + '_epochs' # Where to store model, loss curves, confusion matrix, etc.\n",
    "\n",
    "SAVE_MODEL = True\n",
    "SAVE_OPT = True\n",
    "SAVE_CONFIG = True\n",
    "\n",
    "CUSTOM_PARAMS = {} # Used in case you want to use custom hyperparameters; otherwise, hyperparameters are imported from model_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abf7d1bc-4283-4a8b-9fc0-9c107098bcad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split dataset into train/val/test sets\n",
    "# If MIXED=FALSE: use SPLIT_RATIO = '0.XX, 0.XX'; this will create train/val sets only using peptides in db_file and make test set only polymers\n",
    "# If MIXED=TRUE: use SPLIT_RATIO = '0.XX, 0.XX, 0.XX'; this will mix peptides and polymers into train/val/test sets. Polymers sampled from the same distribution are assigned to the same set. \n",
    "split_db = split(db_file, SEED, SPLIT_RATIO, MIXED = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d408f978-b809-4a37-b81a-d250d37a2986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale dataset using only node and edge features in train set\n",
    "features = scale(split_db['train'], SMILES, DESCRIPTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68365074-cc1c-4a10-b39e-095b8f115982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "dataset = DataSet(db_file, features, split_db, LABELNAME, TASK, MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2167f84-5750-403b-9466-91eeaa8e36eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize classifier and run\n",
    "test = multiclass_NN(dataset, MODEL, NUM_EPOCHS, NUM_WORKERS, DESCRIPTORS, CUSTOM_PARAMS, MODEL_PATH, SAVE_MODEL, SAVE_OPT, SAVE_CONFIG)\n",
    "test.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2b836b7-f60c-48db-a05c-ac3b684e45cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump feature dictionary (needed for inference)\n",
    "joblib.dump(features, MODEL_PATH + '/features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94705bb9-21ef-46fc-9f62-0b4404a7da47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
